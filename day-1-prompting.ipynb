{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c454f6b6",
   "metadata": {
    "id": "jkxRSYjzA1oX",
    "papermill": {
     "duration": 0.009217,
     "end_time": "2025-04-07T21:04:47.091594",
     "exception": false,
     "start_time": "2025-04-07T21:04:47.082377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Copyright 2025 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd38114",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-04-07T21:04:47.110728Z",
     "iopub.status.busy": "2025-04-07T21:04:47.110398Z",
     "iopub.status.idle": "2025-04-07T21:04:47.114100Z",
     "shell.execute_reply": "2025-04-07T21:04:47.113577Z"
    },
    "id": "5u5OZ2ShA3BA",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.014369,
     "end_time": "2025-04-07T21:04:47.115331",
     "exception": false,
     "start_time": "2025-04-07T21:04:47.100962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e3c13",
   "metadata": {
    "id": "csNPnkuCobmG",
    "papermill": {
     "duration": 0.007978,
     "end_time": "2025-04-07T21:04:47.131548",
     "exception": false,
     "start_time": "2025-04-07T21:04:47.123570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Day 1 - Prompting\n",
    "\n",
    "Welcome to the Kaggle 5-day Generative AI course!\n",
    "\n",
    "This notebook will show you how to get started with the Gemini API and walk you through some of the example prompts and techniques that you can also read about in the Prompting whitepaper. You don't need to read the whitepaper to use this notebook, but the papers will give you some theoretical context and background to complement this interactive notebook.\n",
    "\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "In this notebook, you'll start exploring prompting using the Python SDK and AI Studio. For some inspiration, you might enjoy exploring some apps that have been built using the Gemini family of models. Here are a few that we like, and we think you will too.\n",
    "\n",
    "* [TextFX](https://textfx.withgoogle.com/) is a suite of AI-powered tools for rappers, made in collaboration with Lupe Fiasco,\n",
    "* [SQL Talk](https://sql-talk-r5gdynozbq-uc.a.run.app/) shows how you can talk directly to a database using the Gemini API,\n",
    "* [NotebookLM](https://notebooklm.google/) uses Gemini models to build your own personal AI research assistant.\n",
    "\n",
    "\n",
    "## For help\n",
    "\n",
    "**Common issues are covered in the [FAQ and troubleshooting guide](https://www.kaggle.com/code/markishere/day-0-troubleshooting-and-faqs).**\n",
    "\n",
    "## New for Gemini 2.0!\n",
    "\n",
    "This course material was first launched in November 2024. The AI and LLM space is moving incredibly fast, so we have made some updates to use the latest models and capabilities.\n",
    "\n",
    "* These codelabs have been updated to use the Gemini 2.0 family of models.\n",
    "* The Python SDK has been updated from `google-generativeai` to the new, unified [`google-genai`](https://pypi.org/project/google-genai) SDK.\n",
    "  * This new SDK works with both the developer Gemini API as well as Google Cloud Vertex AI, and switching is [as simple as changing some fields](https://pypi.org/project/google-genai/#:~:text=.Client%28%29-,API%20Selection,-By%20default%2C%20the).\n",
    "* New model capabilities have been added to the relevant codelabs, such as \"thinking mode\" in this lab.\n",
    "* Day 1 includes a new [Evaluation codelab](https://www.kaggle.com/code/markishere/day-1-evaluation-and-structured-output)."
   ]
  },
  {
   "attachments": {
    "d8299597-f771-4195-ac94-1d6f658ef2bd.png": {
     "image/png": "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"
    },
    "ed25e346-2e15-4ab0-bc22-503eb1845e66.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "caae279e",
   "metadata": {
    "id": "f00ea7c99e44",
    "papermill": {
     "duration": 0.007975,
     "end_time": "2025-04-07T21:04:47.147724",
     "exception": false,
     "start_time": "2025-04-07T21:04:47.139749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get started with Kaggle notebooks\n",
    "\n",
    "If this is your first time using a Kaggle notebook, welcome! You can read about how to use Kaggle notebooks [in the docs](https://www.kaggle.com/docs/notebooks).\n",
    "\n",
    "First, you will need to phone verify your account at kaggle.com/settings.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14992fc0",
   "metadata": {
    "papermill": {
     "duration": 0.008035,
     "end_time": "2025-04-07T21:04:47.164021",
     "exception": false,
     "start_time": "2025-04-07T21:04:47.155986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To run this notebook, as well as the others in this course, you will need to make a copy, or fork, the notebook. Look for the `Copy and Edit` button in the top-right, and **click it** to make an editable, private copy of the notebook. It should look like this one:\n",
    "\n",
    "![Copy and Edit button](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_1.png)\n",
    "\n",
    "Your copy will now have a ‚ñ∂Ô∏è **Run** button next to each code cell that you can press to execute that cell. These notebooks are expected to be run in order from top-to-bottom, but you are encouraged to add new cells, run your own code and explore. If you get stuck, you can try the `Factory reset` option in the `Run` menu, or head back to the original notebook and make a fresh copy.\n",
    "\n",
    "![Run cell button](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_2.png)\n",
    "\n",
    "### Problems?\n",
    "\n",
    "If you have any problems, head over to the [Kaggle Discord](https://discord.com/invite/kaggle), find the [`#5dgai-q-and-a` channel](https://discord.com/channels/1101210829807956100/1303438695143178251) and ask for help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa413d51",
   "metadata": {
    "id": "ExkOXcPxtTb5",
    "papermill": {
     "duration": 0.00816,
     "end_time": "2025-04-07T21:04:47.180361",
     "exception": false,
     "start_time": "2025-04-07T21:04:47.172201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get started with the Gemini API\n",
    "\n",
    "All of the exercises in this notebook will use the [Gemini API](https://ai.google.dev/gemini-api/) by way of the [Python SDK](https://pypi.org/project/google-genai/). Each of these prompts can be accessed directly in [Google AI Studio](https://aistudio.google.com/) too, so if you would rather use a web interface and skip the code for this activity, look for the <img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> AI Studio link on each prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4aec3f",
   "metadata": {
    "papermill": {
     "duration": 0.008098,
     "end_time": "2025-04-07T21:04:47.197628",
     "exception": false,
     "start_time": "2025-04-07T21:04:47.189530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, you will need to add your API key to your Kaggle Notebook as a Kaggle User Secret.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_1.png)\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_2.png)\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_3.png)\n",
    "![](https://storage.googleapis.com/kaggle-media/Images/5dgai_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6b191",
   "metadata": {
    "id": "UAjUV3BsvFXQ",
    "papermill": {
     "duration": 0.007869,
     "end_time": "2025-04-07T21:04:47.213680",
     "exception": false,
     "start_time": "2025-04-07T21:04:47.205811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90d808a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:04:47.231086Z",
     "iopub.status.busy": "2025-04-07T21:04:47.230815Z",
     "iopub.status.idle": "2025-04-07T21:04:59.615994Z",
     "shell.execute_reply": "2025-04-07T21:04:59.615109Z"
    },
    "id": "NzwzJFU9LqkJ",
    "papermill": {
     "duration": 12.396026,
     "end_time": "2025-04-07T21:04:59.617796",
     "exception": false,
     "start_time": "2025-04-07T21:04:47.221770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n",
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db76db",
   "metadata": {
    "papermill": {
     "duration": 0.007961,
     "end_time": "2025-04-07T21:04:59.634604",
     "exception": false,
     "start_time": "2025-04-07T21:04:59.626643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Import the SDK and some helpers for rendering the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ddd07f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:04:59.652405Z",
     "iopub.status.busy": "2025-04-07T21:04:59.652122Z",
     "iopub.status.idle": "2025-04-07T21:05:00.567968Z",
     "shell.execute_reply": "2025-04-07T21:05:00.567016Z"
    },
    "id": "5DwxYIRavMST",
    "papermill": {
     "duration": 0.927118,
     "end_time": "2025-04-07T21:05:00.569873",
     "exception": false,
     "start_time": "2025-04-07T21:04:59.642755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f285e",
   "metadata": {
    "papermill": {
     "duration": 0.007914,
     "end_time": "2025-04-07T21:05:00.586488",
     "exception": false,
     "start_time": "2025-04-07T21:05:00.578574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Set up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6247a44b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:00.604098Z",
     "iopub.status.busy": "2025-04-07T21:05:00.603726Z",
     "iopub.status.idle": "2025-04-07T21:05:00.738053Z",
     "shell.execute_reply": "2025-04-07T21:05:00.737304Z"
    },
    "papermill": {
     "duration": 0.145293,
     "end_time": "2025-04-07T21:05:00.739851",
     "exception": false,
     "start_time": "2025-04-07T21:05:00.594558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a77a68",
   "metadata": {
    "id": "DNEt2BCOvOJ1",
    "papermill": {
     "duration": 0.007932,
     "end_time": "2025-04-07T21:05:00.756828",
     "exception": false,
     "start_time": "2025-04-07T21:05:00.748896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set up your API key\n",
    "\n",
    "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
    "\n",
    "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa9de4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:00.774526Z",
     "iopub.status.busy": "2025-04-07T21:05:00.774110Z",
     "iopub.status.idle": "2025-04-07T21:05:00.950226Z",
     "shell.execute_reply": "2025-04-07T21:05:00.949519Z"
    },
    "id": "SHl0bkPCvayd",
    "papermill": {
     "duration": 0.186984,
     "end_time": "2025-04-07T21:05:00.951990",
     "exception": false,
     "start_time": "2025-04-07T21:05:00.765006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "attachments": {
    "d2be9aab-2527-48af-b6cf-1f06cdd7c3a7.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "bc381ea6",
   "metadata": {
    "id": "4e720472fd86",
    "papermill": {
     "duration": 0.008078,
     "end_time": "2025-04-07T21:05:00.968902",
     "exception": false,
     "start_time": "2025-04-07T21:05:00.960824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n",
    "\n",
    "![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00692c",
   "metadata": {
    "id": "H_YXCYIKvyZJ",
    "papermill": {
     "duration": 0.008165,
     "end_time": "2025-04-07T21:05:00.985307",
     "exception": false,
     "start_time": "2025-04-07T21:05:00.977142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Run your first prompt\n",
    "\n",
    "In this step, you will test that your API key is set up correctly by making a request.\n",
    "\n",
    "The Python SDK uses a [`Client` object](https://googleapis.github.io/python-genai/genai.html#genai.client.Client) to make requests to the API. The client lets you control which back-end to use (between the Gemini API and Vertex AI) and handles authentication (the API key).\n",
    "\n",
    "The `gemini-2.0-flash` model has been selected here.\n",
    "\n",
    "**Note**: If you see a `TransportError` on this step, you may need to **üîÅ Factory reset** the notebook one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ccd621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:01.003182Z",
     "iopub.status.busy": "2025-04-07T21:05:01.002920Z",
     "iopub.status.idle": "2025-04-07T21:05:11.986635Z",
     "shell.execute_reply": "2025-04-07T21:05:11.985932Z"
    },
    "id": "BV1o0PmcvyJF",
    "papermill": {
     "duration": 10.994249,
     "end_time": "2025-04-07T21:05:11.988084",
     "exception": false,
     "start_time": "2025-04-07T21:05:00.993835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Retrieval-Augmented Generation (RAG) technique combines the strengths of retrieval-based and generation-based approaches for natural language processing. It allows language models to access and incorporate external knowledge sources, leading to more accurate, informative, and contextually relevant responses. Here's a breakdown of the steps involved in implementing RAG:\n",
      "\n",
      "**1. Data Preparation and Indexing:**\n",
      "\n",
      "*   **Gather Data:**\n",
      "    *   **Identify Relevant Knowledge Sources:** Determine the knowledge domains relevant to your application (e.g., documentation, knowledge bases, web pages, research papers).\n",
      "    *   **Extract Data:**  Extract the information from these sources. This might involve:\n",
      "        *   Web scraping for online content.\n",
      "        *   Reading files (PDFs, TXT, DOCX, etc.).\n",
      "        *   Querying databases.\n",
      "        *   Using APIs to access structured data.\n",
      "\n",
      "*   **Chunking:**\n",
      "    *   **Divide Data into Manageable Chunks:** Break down the extracted data into smaller, meaningful segments.  Careful chunking is *crucial* for retrieval performance.  Consider these factors:\n",
      "        *   **Chunk Size:** Smaller chunks capture more granular details but can lose context. Larger chunks retain context but may retrieve irrelevant information.  Experiment to find the optimal size for your data and use case.  Common sizes are around 100-500 tokens.\n",
      "        *   **Chunking Strategies:**\n",
      "            *   **Fixed-Size Chunking:** Simple, but may split sentences or paragraphs awkwardly.\n",
      "            *   **Semantic Chunking:** Uses sentence segmentation or other NLP techniques to ensure chunks are semantically coherent.\n",
      "            *   **Recursive Chunking:** Divides text recursively based on headings, paragraphs, and sentences. This is often a very good strategy for retaining hierarchies.\n",
      "            *   **Overlapping Chunks:** Include some overlap between chunks to maintain continuity, especially when using smaller chunks.\n",
      "\n",
      "*   **Embeddings:**\n",
      "    *   **Create Vector Embeddings:**  Convert each chunk into a vector representation using a suitable embedding model (e.g., Sentence Transformers, OpenAI's `text-embedding-ada-002`, or other pre-trained or fine-tuned models).  The embedding model should be chosen based on the characteristics of your data and the type of queries you expect.\n",
      "    *   **Embedding Quality:** The quality of your embeddings directly impacts retrieval accuracy.  Choose a model trained on data similar to your knowledge base and consider fine-tuning it.\n",
      "\n",
      "*   **Indexing:**\n",
      "    *   **Store Embeddings in a Vector Database (Vector Store):**  Use a vector database to efficiently store and index the generated embeddings.  Popular options include:\n",
      "        *   **Pinecone:** Scalable and performant cloud-based vector database.\n",
      "        *   **Weaviate:** Open-source vector search engine with graph capabilities.\n",
      "        *   **ChromaDB:** Open-source embedding database.\n",
      "        *   **FAISS (Facebook AI Similarity Search):**  Library for efficient similarity search and clustering of dense vectors.\n",
      "        *   **Milvus:** Open-source vector database built for AI applications.\n",
      "    *   **Populate the Vector Database:** Load the embeddings and corresponding text chunks into the vector database. Configure the database for efficient similarity search.\n",
      "\n",
      "**2. Query and Retrieval:**\n",
      "\n",
      "*   **Formulate Query:**  The user inputs a question or query.\n",
      "\n",
      "*   **Embed Query:** Convert the user's query into a vector embedding using the *same embedding model* used to embed the data chunks.  This consistency is vital for accurate similarity search.\n",
      "\n",
      "*   **Similarity Search:**  Perform a similarity search in the vector database to find the chunks whose embeddings are most similar to the query embedding. The vector database uses similarity metrics like cosine similarity or dot product to identify the top `k` most relevant chunks.\n",
      "\n",
      "*   **Retrieve Relevant Chunks:**  Retrieve the top `k` chunks of text and their corresponding metadata (e.g., source document, title) from the vector database.  The value of `k` is a hyperparameter that you can tune. A higher `k` might retrieve more relevant information but can also introduce irrelevant information.\n",
      "\n",
      "**3. Generation:**\n",
      "\n",
      "*   **Context Augmentation:** Combine the retrieved chunks with the original query to create a context-augmented prompt.  This is the *core* of the RAG process.  The goal is to provide the language model with the relevant knowledge it needs to generate a high-quality response. Common approaches include:\n",
      "    *   **Simple Concatenation:**  Simply concatenate the retrieved chunks with the query.\n",
      "    *   **Prompt Engineering:** Craft a more sophisticated prompt that explicitly instructs the language model to use the retrieved information to answer the query.  For example:\n",
      "        *   \"Answer the following question based on the context provided. Question: {query} Context: {retrieved_chunks}\"\n",
      "        *   \"Use the following information to answer the question: {query}. Information: {retrieved_chunks}\"\n",
      "\n",
      "*   **Generate Response:**  Feed the context-augmented prompt to a large language model (LLM) like GPT-3.5, GPT-4, Llama 2, PaLM 2, or others.  The LLM uses its pre-trained knowledge and the provided context to generate a response to the query.\n",
      "\n",
      "*   **Response Refinement (Optional):**  Optionally, refine the generated response using techniques like:\n",
      "    *   **Post-processing:** Correct grammar, spelling, and punctuation errors.\n",
      "    *   **Filtering:**  Remove irrelevant or redundant information.\n",
      "    *   **Summarization:** Condense the response for brevity.\n",
      "\n",
      "**4. Evaluation and Iteration:**\n",
      "\n",
      "*   **Evaluate Performance:**  Evaluate the quality of the RAG system's responses using metrics such as:\n",
      "    *   **Relevance:** How well the response addresses the query.\n",
      "    *   **Accuracy:** How factually correct the response is.\n",
      "    *   **Coherence:** How well the response is written and organized.\n",
      "    *   **Groundedness:** How well the response is supported by the retrieved context.\n",
      "    *   **Hallucination:** Assessing if the LLM is making up facts.\n",
      "\n",
      "*   **Iterate and Refine:** Based on the evaluation results, iterate on the following aspects of the RAG system:\n",
      "    *   **Chunking Strategy:** Adjust chunk size and chunking method.\n",
      "    *   **Embedding Model:** Experiment with different embedding models or fine-tune the existing model.\n",
      "    *   **Vector Database:** Explore different vector databases and indexing techniques.\n",
      "    *   **Retrieval Strategy:** Adjust the value of `k` (number of retrieved chunks).\n",
      "    *   **Prompt Engineering:** Refine the prompt to better guide the LLM.\n",
      "    *   **LLM:** Experiment with different LLMs or fine-tune the chosen LLM.\n",
      "\n",
      "**Key Considerations and Best Practices:**\n",
      "\n",
      "*   **Prompt Engineering is Critical:**  The prompt is the interface between the retrieved context and the language model. Well-crafted prompts can significantly improve the quality of the generated responses.\n",
      "*   **Context Window Limits:**  Language models have a limited context window (the amount of text they can process at once).  Ensure that the combined length of the query and retrieved chunks does not exceed the LLM's context window.  Consider techniques like truncation or summarization if necessary.\n",
      "*   **Data Quality Matters:**  The quality of the data used to build the knowledge base directly impacts the performance of the RAG system.  Clean and accurate data is essential.\n",
      "*   **Trade-offs Between Retrieval and Generation:**  RAG balances the benefits of retrieval and generation.  Retrieval provides factual grounding, while generation provides fluency and expressiveness.  Finding the right balance is crucial for achieving optimal performance.\n",
      "*   **Metadata Management:** Include relevant metadata (source document, timestamps, etc.) with each chunk. This metadata can be used for filtering and improving the retrieval process.\n",
      "*   **Hybrid Retrieval Methods:** Consider combining vector search with other retrieval methods, such as keyword search or semantic search, to improve accuracy.\n",
      "*   **Continuous Monitoring:** Continuously monitor the performance of the RAG system and retrain or update the knowledge base as needed.\n",
      "*   **Experimentation is Key:** RAG is a highly adaptable technique. Don't be afraid to experiment with different approaches to find the best configuration for your specific use case.\n",
      "*   **Security:** Be aware of potential security vulnerabilities, such as prompt injection attacks, and implement appropriate security measures. Validate and sanitize user inputs and consider implementing guardrails to prevent the LLM from generating harmful or inappropriate content.\n",
      "\n",
      "By following these steps and continuously refining your approach, you can build a powerful and effective RAG system that leverages external knowledge to generate high-quality responses. Remember that the optimal configuration will depend on your specific data, use case, and the capabilities of the chosen LLM.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Explain the steps to implement Rag technique.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3f57ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:12.006241Z",
     "iopub.status.busy": "2025-04-07T21:05:12.005958Z",
     "iopub.status.idle": "2025-04-07T21:05:20.943751Z",
     "shell.execute_reply": "2025-04-07T21:05:20.942906Z"
    },
    "papermill": {
     "duration": 8.948401,
     "end_time": "2025-04-07T21:05:20.945162",
     "exception": false,
     "start_time": "2025-04-07T21:05:11.996761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, storing chatbot responses and related data in a database is generally a good practice for several reasons. It allows you to:\n",
      "\n",
      "*   **Improve the chatbot:** Analyze user interactions to identify areas where the chatbot struggles, refine its responses, and train it on new data.\n",
      "*   **Personalize the user experience:** Store user preferences and conversation history to provide more relevant and personalized responses.\n",
      "*   **Track performance:** Monitor key metrics like user engagement, conversation length, and task completion rates to assess the chatbot's effectiveness.\n",
      "*   **Ensure compliance:** Maintain a record of user interactions to comply with data privacy regulations and legal requirements.\n",
      "*   **Troubleshoot issues:** Investigate and resolve technical problems by examining conversation logs and error messages.\n",
      "*   **Generate reports:** Create reports on chatbot usage patterns, user feedback, and other relevant metrics.\n",
      "\n",
      "Here's a breakdown of the data you should consider storing in the database:\n",
      "\n",
      "**1. User Information:**\n",
      "\n",
      "*   **User ID:** A unique identifier for each user (if available, can be an internal user ID or a hashed representation of their email/phone number).\n",
      "*   **Demographic data:** (Optional, but useful for segmentation) Age, gender, location, language, etc.  **Important:** Be mindful of privacy regulations and only collect data that is necessary and with user consent.\n",
      "*   **Registration date/time:** When the user first interacted with the chatbot.\n",
      "\n",
      "**2. Conversation Data:**\n",
      "\n",
      "*   **Conversation ID:** A unique identifier for each conversation session.\n",
      "*   **Timestamp:** The date and time of each message.\n",
      "*   **User Input:** The exact text or data the user sent to the chatbot.\n",
      "*   **Chatbot Response:** The text or data the chatbot sent to the user.\n",
      "*   **Intent:** The identified intent of the user's message (e.g., \"book_flight\", \"check_weather\"). This requires intent recognition to be built into your chatbot.\n",
      "*   **Entities:** The specific data extracted from the user's message (e.g., \"New York\" as the destination, \"tomorrow\" as the date).  Requires entity recognition.\n",
      "*   **Confidence Score:** The confidence level of the intent and entity recognition.  This is very helpful for identifying uncertain or ambiguous user input.\n",
      "*   **Context:** The current state of the conversation (e.g., which question the chatbot is currently asking, which step the user is on in a multi-step process).  Critical for maintaining context across multiple turns.\n",
      "*   **Feedback:**  User feedback on the chatbot's responses (e.g., thumbs up/down, rating, or a free-text comment). This is vital for improving chatbot performance.\n",
      "*   **Channel:** The platform used for the conversation (e.g., web chat, Facebook Messenger, Slack).\n",
      "*   **Error Messages:** Any error messages encountered during the conversation.\n",
      "*   **API Calls & Results:** If the chatbot interacts with external APIs, store the API requests and responses.\n",
      "*   **Assigned Agent (if applicable):** If a human agent takes over the conversation, record the agent's ID.\n",
      "*   **Conversation Resolution:**  How the conversation ended (e.g., \"successfully completed task,\" \"user abandoned,\" \"transferred to agent\").\n",
      "*   **User satisfaction:** A metric indicating the user's satisfaction with the conversation (e.g., a rating scale, sentiment analysis of their messages).\n",
      "\n",
      "**3. Chatbot Performance Data:**\n",
      "\n",
      "*   **Conversation Length:** The number of messages exchanged in a conversation.\n",
      "*   **Conversation Duration:** The total time of the conversation.\n",
      "*   **Task Completion Rate:** The percentage of conversations that successfully complete a desired task.\n",
      "*   **Fallback Rate:** The percentage of times the chatbot fails to understand the user's intent and resorts to a fallback response.  High fallback rates indicate areas for improvement.\n",
      "*   **Containment Rate:** The percentage of conversations that are resolved without human intervention.\n",
      "\n",
      "**Database Considerations:**\n",
      "\n",
      "*   **Database Type:** Choose a database that is suitable for storing large volumes of text data and supporting complex queries.  Popular options include:\n",
      "    *   **Relational Databases (SQL):** PostgreSQL, MySQL.  Good for structured data and complex queries.\n",
      "    *   **NoSQL Databases:** MongoDB (document database), Cassandra (column family database).  Good for unstructured data and scalability.\n",
      "*   **Data Modeling:** Design the database schema carefully to ensure data integrity and efficient querying.\n",
      "*   **Indexing:** Use indexes to speed up queries.\n",
      "*   **Data Security:** Implement appropriate security measures to protect sensitive user data.\n",
      "*   **Data Retention Policy:** Define a policy for how long data will be stored. Comply with privacy regulations.\n",
      "\n",
      "**Example Table Structure (Simplified, using PostgreSQL):**\n",
      "\n",
      "```sql\n",
      "-- Users Table\n",
      "CREATE TABLE users (\n",
      "    user_id SERIAL PRIMARY KEY,\n",
      "    username VARCHAR(255),\n",
      "    -- other user information\n",
      "    created_at TIMESTAMP DEFAULT NOW()\n",
      ");\n",
      "\n",
      "-- Conversations Table\n",
      "CREATE TABLE conversations (\n",
      "    conversation_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
      "    user_id INTEGER REFERENCES users(user_id),\n",
      "    start_time TIMESTAMP DEFAULT NOW(),\n",
      "    end_time TIMESTAMP,\n",
      "    resolution VARCHAR(255)\n",
      ");\n",
      "\n",
      "-- Messages Table\n",
      "CREATE TABLE messages (\n",
      "    message_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
      "    conversation_id UUID REFERENCES conversations(conversation_id),\n",
      "    sender VARCHAR(50) CHECK (sender IN ('user', 'bot')),\n",
      "    message_text TEXT,\n",
      "    intent VARCHAR(255),\n",
      "    entities JSONB,\n",
      "    timestamp TIMESTAMP DEFAULT NOW()\n",
      ");\n",
      "\n",
      "-- Feedback Table\n",
      "CREATE TABLE feedback (\n",
      "    feedback_id SERIAL PRIMARY KEY,\n",
      "    conversation_id UUID REFERENCES conversations(conversation_id),\n",
      "    user_rating INTEGER,\n",
      "    comment TEXT,\n",
      "    timestamp TIMESTAMP DEFAULT NOW()\n",
      ");\n",
      "```\n",
      "\n",
      "**Important Considerations for GDPR and Privacy:**\n",
      "\n",
      "*   **Data Minimization:** Only collect data that is strictly necessary for the purposes you have defined.\n",
      "*   **User Consent:** Obtain explicit consent from users before collecting and storing their personal data.  This is especially important for demographic information.\n",
      "*   **Transparency:** Be transparent about how you are collecting and using user data.  Provide a clear privacy policy.\n",
      "*   **Right to Access:** Provide users with the ability to access and correct their personal data.\n",
      "*   **Right to Erasure (Right to be Forgotten):** Allow users to request that their personal data be deleted.\n",
      "*   **Data Security:** Implement strong security measures to protect user data from unauthorized access or disclosure.\n",
      "*   **Anonymization/Pseudonymization:** Consider anonymizing or pseudonymizing data to reduce the risk of re-identification.\n",
      "\n",
      "By carefully considering these factors, you can design a database that effectively stores chatbot data, enabling you to improve the chatbot's performance, personalize the user experience, and ensure compliance with data privacy regulations. Remember to prioritize user privacy and be transparent about your data collection practices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response2=client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Is the responses of the chatbot should be stored in DB and which data should be stored?\")\n",
    "print(response2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6666c941",
   "metadata": {
    "id": "f60ed9d8ae41",
    "papermill": {
     "duration": 0.008507,
     "end_time": "2025-04-07T21:05:20.962676",
     "exception": false,
     "start_time": "2025-04-07T21:05:20.954169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The response often comes back in markdown format, which you can render directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "770b13cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:20.980821Z",
     "iopub.status.busy": "2025-04-07T21:05:20.980550Z",
     "iopub.status.idle": "2025-04-07T21:05:20.986303Z",
     "shell.execute_reply": "2025-04-07T21:05:20.985678Z"
    },
    "id": "c933e5e460a5",
    "papermill": {
     "duration": 0.01654,
     "end_time": "2025-04-07T21:05:20.987699",
     "exception": false,
     "start_time": "2025-04-07T21:05:20.971159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Retrieval-Augmented Generation (RAG) technique combines the strengths of retrieval-based and generation-based approaches for natural language processing. It allows language models to access and incorporate external knowledge sources, leading to more accurate, informative, and contextually relevant responses. Here's a breakdown of the steps involved in implementing RAG:\n",
       "\n",
       "**1. Data Preparation and Indexing:**\n",
       "\n",
       "*   **Gather Data:**\n",
       "    *   **Identify Relevant Knowledge Sources:** Determine the knowledge domains relevant to your application (e.g., documentation, knowledge bases, web pages, research papers).\n",
       "    *   **Extract Data:**  Extract the information from these sources. This might involve:\n",
       "        *   Web scraping for online content.\n",
       "        *   Reading files (PDFs, TXT, DOCX, etc.).\n",
       "        *   Querying databases.\n",
       "        *   Using APIs to access structured data.\n",
       "\n",
       "*   **Chunking:**\n",
       "    *   **Divide Data into Manageable Chunks:** Break down the extracted data into smaller, meaningful segments.  Careful chunking is *crucial* for retrieval performance.  Consider these factors:\n",
       "        *   **Chunk Size:** Smaller chunks capture more granular details but can lose context. Larger chunks retain context but may retrieve irrelevant information.  Experiment to find the optimal size for your data and use case.  Common sizes are around 100-500 tokens.\n",
       "        *   **Chunking Strategies:**\n",
       "            *   **Fixed-Size Chunking:** Simple, but may split sentences or paragraphs awkwardly.\n",
       "            *   **Semantic Chunking:** Uses sentence segmentation or other NLP techniques to ensure chunks are semantically coherent.\n",
       "            *   **Recursive Chunking:** Divides text recursively based on headings, paragraphs, and sentences. This is often a very good strategy for retaining hierarchies.\n",
       "            *   **Overlapping Chunks:** Include some overlap between chunks to maintain continuity, especially when using smaller chunks.\n",
       "\n",
       "*   **Embeddings:**\n",
       "    *   **Create Vector Embeddings:**  Convert each chunk into a vector representation using a suitable embedding model (e.g., Sentence Transformers, OpenAI's `text-embedding-ada-002`, or other pre-trained or fine-tuned models).  The embedding model should be chosen based on the characteristics of your data and the type of queries you expect.\n",
       "    *   **Embedding Quality:** The quality of your embeddings directly impacts retrieval accuracy.  Choose a model trained on data similar to your knowledge base and consider fine-tuning it.\n",
       "\n",
       "*   **Indexing:**\n",
       "    *   **Store Embeddings in a Vector Database (Vector Store):**  Use a vector database to efficiently store and index the generated embeddings.  Popular options include:\n",
       "        *   **Pinecone:** Scalable and performant cloud-based vector database.\n",
       "        *   **Weaviate:** Open-source vector search engine with graph capabilities.\n",
       "        *   **ChromaDB:** Open-source embedding database.\n",
       "        *   **FAISS (Facebook AI Similarity Search):**  Library for efficient similarity search and clustering of dense vectors.\n",
       "        *   **Milvus:** Open-source vector database built for AI applications.\n",
       "    *   **Populate the Vector Database:** Load the embeddings and corresponding text chunks into the vector database. Configure the database for efficient similarity search.\n",
       "\n",
       "**2. Query and Retrieval:**\n",
       "\n",
       "*   **Formulate Query:**  The user inputs a question or query.\n",
       "\n",
       "*   **Embed Query:** Convert the user's query into a vector embedding using the *same embedding model* used to embed the data chunks.  This consistency is vital for accurate similarity search.\n",
       "\n",
       "*   **Similarity Search:**  Perform a similarity search in the vector database to find the chunks whose embeddings are most similar to the query embedding. The vector database uses similarity metrics like cosine similarity or dot product to identify the top `k` most relevant chunks.\n",
       "\n",
       "*   **Retrieve Relevant Chunks:**  Retrieve the top `k` chunks of text and their corresponding metadata (e.g., source document, title) from the vector database.  The value of `k` is a hyperparameter that you can tune. A higher `k` might retrieve more relevant information but can also introduce irrelevant information.\n",
       "\n",
       "**3. Generation:**\n",
       "\n",
       "*   **Context Augmentation:** Combine the retrieved chunks with the original query to create a context-augmented prompt.  This is the *core* of the RAG process.  The goal is to provide the language model with the relevant knowledge it needs to generate a high-quality response. Common approaches include:\n",
       "    *   **Simple Concatenation:**  Simply concatenate the retrieved chunks with the query.\n",
       "    *   **Prompt Engineering:** Craft a more sophisticated prompt that explicitly instructs the language model to use the retrieved information to answer the query.  For example:\n",
       "        *   \"Answer the following question based on the context provided. Question: {query} Context: {retrieved_chunks}\"\n",
       "        *   \"Use the following information to answer the question: {query}. Information: {retrieved_chunks}\"\n",
       "\n",
       "*   **Generate Response:**  Feed the context-augmented prompt to a large language model (LLM) like GPT-3.5, GPT-4, Llama 2, PaLM 2, or others.  The LLM uses its pre-trained knowledge and the provided context to generate a response to the query.\n",
       "\n",
       "*   **Response Refinement (Optional):**  Optionally, refine the generated response using techniques like:\n",
       "    *   **Post-processing:** Correct grammar, spelling, and punctuation errors.\n",
       "    *   **Filtering:**  Remove irrelevant or redundant information.\n",
       "    *   **Summarization:** Condense the response for brevity.\n",
       "\n",
       "**4. Evaluation and Iteration:**\n",
       "\n",
       "*   **Evaluate Performance:**  Evaluate the quality of the RAG system's responses using metrics such as:\n",
       "    *   **Relevance:** How well the response addresses the query.\n",
       "    *   **Accuracy:** How factually correct the response is.\n",
       "    *   **Coherence:** How well the response is written and organized.\n",
       "    *   **Groundedness:** How well the response is supported by the retrieved context.\n",
       "    *   **Hallucination:** Assessing if the LLM is making up facts.\n",
       "\n",
       "*   **Iterate and Refine:** Based on the evaluation results, iterate on the following aspects of the RAG system:\n",
       "    *   **Chunking Strategy:** Adjust chunk size and chunking method.\n",
       "    *   **Embedding Model:** Experiment with different embedding models or fine-tune the existing model.\n",
       "    *   **Vector Database:** Explore different vector databases and indexing techniques.\n",
       "    *   **Retrieval Strategy:** Adjust the value of `k` (number of retrieved chunks).\n",
       "    *   **Prompt Engineering:** Refine the prompt to better guide the LLM.\n",
       "    *   **LLM:** Experiment with different LLMs or fine-tune the chosen LLM.\n",
       "\n",
       "**Key Considerations and Best Practices:**\n",
       "\n",
       "*   **Prompt Engineering is Critical:**  The prompt is the interface between the retrieved context and the language model. Well-crafted prompts can significantly improve the quality of the generated responses.\n",
       "*   **Context Window Limits:**  Language models have a limited context window (the amount of text they can process at once).  Ensure that the combined length of the query and retrieved chunks does not exceed the LLM's context window.  Consider techniques like truncation or summarization if necessary.\n",
       "*   **Data Quality Matters:**  The quality of the data used to build the knowledge base directly impacts the performance of the RAG system.  Clean and accurate data is essential.\n",
       "*   **Trade-offs Between Retrieval and Generation:**  RAG balances the benefits of retrieval and generation.  Retrieval provides factual grounding, while generation provides fluency and expressiveness.  Finding the right balance is crucial for achieving optimal performance.\n",
       "*   **Metadata Management:** Include relevant metadata (source document, timestamps, etc.) with each chunk. This metadata can be used for filtering and improving the retrieval process.\n",
       "*   **Hybrid Retrieval Methods:** Consider combining vector search with other retrieval methods, such as keyword search or semantic search, to improve accuracy.\n",
       "*   **Continuous Monitoring:** Continuously monitor the performance of the RAG system and retrain or update the knowledge base as needed.\n",
       "*   **Experimentation is Key:** RAG is a highly adaptable technique. Don't be afraid to experiment with different approaches to find the best configuration for your specific use case.\n",
       "*   **Security:** Be aware of potential security vulnerabilities, such as prompt injection attacks, and implement appropriate security measures. Validate and sanitize user inputs and consider implementing guardrails to prevent the LLM from generating harmful or inappropriate content.\n",
       "\n",
       "By following these steps and continuously refining your approach, you can build a powerful and effective RAG system that leverages external knowledge to generate high-quality responses. Remember that the optimal configuration will depend on your specific data, use case, and the capabilities of the chosen LLM.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2fce134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:21.009660Z",
     "iopub.status.busy": "2025-04-07T21:05:21.008920Z",
     "iopub.status.idle": "2025-04-07T21:05:21.014494Z",
     "shell.execute_reply": "2025-04-07T21:05:21.013707Z"
    },
    "papermill": {
     "duration": 0.017813,
     "end_time": "2025-04-07T21:05:21.015947",
     "exception": false,
     "start_time": "2025-04-07T21:05:20.998134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, storing chatbot responses and related data in a database is generally a good practice for several reasons. It allows you to:\n",
       "\n",
       "*   **Improve the chatbot:** Analyze user interactions to identify areas where the chatbot struggles, refine its responses, and train it on new data.\n",
       "*   **Personalize the user experience:** Store user preferences and conversation history to provide more relevant and personalized responses.\n",
       "*   **Track performance:** Monitor key metrics like user engagement, conversation length, and task completion rates to assess the chatbot's effectiveness.\n",
       "*   **Ensure compliance:** Maintain a record of user interactions to comply with data privacy regulations and legal requirements.\n",
       "*   **Troubleshoot issues:** Investigate and resolve technical problems by examining conversation logs and error messages.\n",
       "*   **Generate reports:** Create reports on chatbot usage patterns, user feedback, and other relevant metrics.\n",
       "\n",
       "Here's a breakdown of the data you should consider storing in the database:\n",
       "\n",
       "**1. User Information:**\n",
       "\n",
       "*   **User ID:** A unique identifier for each user (if available, can be an internal user ID or a hashed representation of their email/phone number).\n",
       "*   **Demographic data:** (Optional, but useful for segmentation) Age, gender, location, language, etc.  **Important:** Be mindful of privacy regulations and only collect data that is necessary and with user consent.\n",
       "*   **Registration date/time:** When the user first interacted with the chatbot.\n",
       "\n",
       "**2. Conversation Data:**\n",
       "\n",
       "*   **Conversation ID:** A unique identifier for each conversation session.\n",
       "*   **Timestamp:** The date and time of each message.\n",
       "*   **User Input:** The exact text or data the user sent to the chatbot.\n",
       "*   **Chatbot Response:** The text or data the chatbot sent to the user.\n",
       "*   **Intent:** The identified intent of the user's message (e.g., \"book_flight\", \"check_weather\"). This requires intent recognition to be built into your chatbot.\n",
       "*   **Entities:** The specific data extracted from the user's message (e.g., \"New York\" as the destination, \"tomorrow\" as the date).  Requires entity recognition.\n",
       "*   **Confidence Score:** The confidence level of the intent and entity recognition.  This is very helpful for identifying uncertain or ambiguous user input.\n",
       "*   **Context:** The current state of the conversation (e.g., which question the chatbot is currently asking, which step the user is on in a multi-step process).  Critical for maintaining context across multiple turns.\n",
       "*   **Feedback:**  User feedback on the chatbot's responses (e.g., thumbs up/down, rating, or a free-text comment). This is vital for improving chatbot performance.\n",
       "*   **Channel:** The platform used for the conversation (e.g., web chat, Facebook Messenger, Slack).\n",
       "*   **Error Messages:** Any error messages encountered during the conversation.\n",
       "*   **API Calls & Results:** If the chatbot interacts with external APIs, store the API requests and responses.\n",
       "*   **Assigned Agent (if applicable):** If a human agent takes over the conversation, record the agent's ID.\n",
       "*   **Conversation Resolution:**  How the conversation ended (e.g., \"successfully completed task,\" \"user abandoned,\" \"transferred to agent\").\n",
       "*   **User satisfaction:** A metric indicating the user's satisfaction with the conversation (e.g., a rating scale, sentiment analysis of their messages).\n",
       "\n",
       "**3. Chatbot Performance Data:**\n",
       "\n",
       "*   **Conversation Length:** The number of messages exchanged in a conversation.\n",
       "*   **Conversation Duration:** The total time of the conversation.\n",
       "*   **Task Completion Rate:** The percentage of conversations that successfully complete a desired task.\n",
       "*   **Fallback Rate:** The percentage of times the chatbot fails to understand the user's intent and resorts to a fallback response.  High fallback rates indicate areas for improvement.\n",
       "*   **Containment Rate:** The percentage of conversations that are resolved without human intervention.\n",
       "\n",
       "**Database Considerations:**\n",
       "\n",
       "*   **Database Type:** Choose a database that is suitable for storing large volumes of text data and supporting complex queries.  Popular options include:\n",
       "    *   **Relational Databases (SQL):** PostgreSQL, MySQL.  Good for structured data and complex queries.\n",
       "    *   **NoSQL Databases:** MongoDB (document database), Cassandra (column family database).  Good for unstructured data and scalability.\n",
       "*   **Data Modeling:** Design the database schema carefully to ensure data integrity and efficient querying.\n",
       "*   **Indexing:** Use indexes to speed up queries.\n",
       "*   **Data Security:** Implement appropriate security measures to protect sensitive user data.\n",
       "*   **Data Retention Policy:** Define a policy for how long data will be stored. Comply with privacy regulations.\n",
       "\n",
       "**Example Table Structure (Simplified, using PostgreSQL):**\n",
       "\n",
       "```sql\n",
       "-- Users Table\n",
       "CREATE TABLE users (\n",
       "    user_id SERIAL PRIMARY KEY,\n",
       "    username VARCHAR(255),\n",
       "    -- other user information\n",
       "    created_at TIMESTAMP DEFAULT NOW()\n",
       ");\n",
       "\n",
       "-- Conversations Table\n",
       "CREATE TABLE conversations (\n",
       "    conversation_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
       "    user_id INTEGER REFERENCES users(user_id),\n",
       "    start_time TIMESTAMP DEFAULT NOW(),\n",
       "    end_time TIMESTAMP,\n",
       "    resolution VARCHAR(255)\n",
       ");\n",
       "\n",
       "-- Messages Table\n",
       "CREATE TABLE messages (\n",
       "    message_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
       "    conversation_id UUID REFERENCES conversations(conversation_id),\n",
       "    sender VARCHAR(50) CHECK (sender IN ('user', 'bot')),\n",
       "    message_text TEXT,\n",
       "    intent VARCHAR(255),\n",
       "    entities JSONB,\n",
       "    timestamp TIMESTAMP DEFAULT NOW()\n",
       ");\n",
       "\n",
       "-- Feedback Table\n",
       "CREATE TABLE feedback (\n",
       "    feedback_id SERIAL PRIMARY KEY,\n",
       "    conversation_id UUID REFERENCES conversations(conversation_id),\n",
       "    user_rating INTEGER,\n",
       "    comment TEXT,\n",
       "    timestamp TIMESTAMP DEFAULT NOW()\n",
       ");\n",
       "```\n",
       "\n",
       "**Important Considerations for GDPR and Privacy:**\n",
       "\n",
       "*   **Data Minimization:** Only collect data that is strictly necessary for the purposes you have defined.\n",
       "*   **User Consent:** Obtain explicit consent from users before collecting and storing their personal data.  This is especially important for demographic information.\n",
       "*   **Transparency:** Be transparent about how you are collecting and using user data.  Provide a clear privacy policy.\n",
       "*   **Right to Access:** Provide users with the ability to access and correct their personal data.\n",
       "*   **Right to Erasure (Right to be Forgotten):** Allow users to request that their personal data be deleted.\n",
       "*   **Data Security:** Implement strong security measures to protect user data from unauthorized access or disclosure.\n",
       "*   **Anonymization/Pseudonymization:** Consider anonymizing or pseudonymizing data to reduce the risk of re-identification.\n",
       "\n",
       "By carefully considering these factors, you can design a database that effectively stores chatbot data, enabling you to improve the chatbot's performance, personalize the user experience, and ensure compliance with data privacy regulations. Remember to prioritize user privacy and be transparent about your data collection practices.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af8189",
   "metadata": {
    "id": "byx0pT9ZMW2Q",
    "papermill": {
     "duration": 0.008711,
     "end_time": "2025-04-07T21:05:21.034153",
     "exception": false,
     "start_time": "2025-04-07T21:05:21.025442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Start a chat\n",
    "\n",
    "The previous example uses a single-turn, text-in/text-out structure, but you can also set up a multi-turn chat structure too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "475bdb92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:21.053159Z",
     "iopub.status.busy": "2025-04-07T21:05:21.052873Z",
     "iopub.status.idle": "2025-04-07T21:05:21.537373Z",
     "shell.execute_reply": "2025-04-07T21:05:21.536265Z"
    },
    "id": "lV_S5ZL5MidD",
    "papermill": {
     "duration": 0.495927,
     "end_time": "2025-04-07T21:05:21.538950",
     "exception": false,
     "start_time": "2025-04-07T21:05:21.043023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greetings, Zlork! It's nice to meet you. How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
    "response = chat.send_message('Hello! My name is Zlork.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0366fcf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:21.559724Z",
     "iopub.status.busy": "2025-04-07T21:05:21.559365Z",
     "iopub.status.idle": "2025-04-07T21:05:23.018162Z",
     "shell.execute_reply": "2025-04-07T21:05:23.016985Z"
    },
    "id": "7b0372c3c64a",
    "papermill": {
     "duration": 1.470703,
     "end_time": "2025-04-07T21:05:23.019668",
     "exception": false,
     "start_time": "2025-04-07T21:05:21.548965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's an interesting fact about dinosaurs:\n",
      "\n",
      "**Many dinosaurs likely had feathers, even some of the largest and most fearsome ones!**\n",
      "\n",
      "While we often picture dinosaurs as scaly reptiles, evidence suggests that feathers were much more widespread than initially thought. This isn't just about small, bird-like dinosaurs; even some theropods (the group that includes *Tyrannosaurus rex*) may have had some feathery coverings, especially when they were younger. These weren't necessarily for flight, but could have served purposes like insulation, display, or camouflage.\n",
      "\n",
      "This discovery has significantly changed our understanding of dinosaur evolution and their connection to birds. It also adds a fascinating level of detail to our mental image of these magnificent creatures!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Can you tell me something interesting about dinosaurs?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c13c966",
   "metadata": {
    "papermill": {
     "duration": 0.0088,
     "end_time": "2025-04-07T21:05:23.037785",
     "exception": false,
     "start_time": "2025-04-07T21:05:23.028985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "While you have the `chat` object alive, the conversation state\n",
    "persists. Confirm that by asking if it knows the user's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d821b363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:23.056990Z",
     "iopub.status.busy": "2025-04-07T21:05:23.056687Z",
     "iopub.status.idle": "2025-04-07T21:05:35.294296Z",
     "shell.execute_reply": "2025-04-07T21:05:35.293163Z"
    },
    "id": "d3f9591392a7",
    "papermill": {
     "duration": 12.24908,
     "end_time": "2025-04-07T21:05:35.295874",
     "exception": false,
     "start_time": "2025-04-07T21:05:23.046794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, integrating a chatbot that uses a BERT model and Retrieval-Augmented Generation (RAG) is a complex but rewarding task. Here's a breakdown of the key steps and considerations involved:\n",
       "\n",
       "**1. Data Preparation & Indexing (for RAG)**\n",
       "\n",
       "*   **Data Collection:** Gather the knowledge base you want your chatbot to access. This could be documents, articles, FAQs, code snippets, or any other relevant text data.\n",
       "*   **Data Cleaning & Preprocessing:** Clean the data to remove irrelevant information (e.g., HTML tags, special characters). You might also need to split large documents into smaller chunks to improve retrieval accuracy. Techniques include sentence splitting or fixed-size chunking.\n",
       "*   **Embedding Generation:** Use BERT (or a similar transformer model) to create embeddings (vector representations) of your text chunks. These embeddings capture the semantic meaning of the text.  You can use a pre-trained BERT model (e.g., `bert-base-uncased`, `sentence-transformers/all-mpnet-base-v2` for better sentence embeddings) or fine-tune one on your specific data if you have enough training examples.\n",
       "*   **Index Creation:** Store the embeddings and their corresponding text chunks in a vector database (also called a vector store).  Popular options include:\n",
       "    *   **FAISS:**  Fast and efficient, especially for large datasets.  Good for local development and deployment.\n",
       "    *   **Annoy:**  Another fast library for approximate nearest neighbor search.\n",
       "    *   **Pinecone:**  A managed vector database service.  Scalable and easy to integrate.\n",
       "    *   **Weaviate:**  An open-source vector database with a GraphQL interface.\n",
       "    *   **ChromaDB:** Another popular open source vector database.\n",
       "    *   **Milvus:**  Another open-source vector database built for AI applications.\n",
       "\n",
       "**2. Chatbot Implementation**\n",
       "\n",
       "*   **User Input Processing:**\n",
       "    *   **Text Input:**  Receive the user's query as text.\n",
       "    *   **Embedding Generation (Query):**  Use the same BERT model you used for your knowledge base to create an embedding of the user's query. This ensures compatibility when searching the vector database.\n",
       "*   **Retrieval:**\n",
       "    *   **Similarity Search:**  Use the query embedding to perform a similarity search in your vector database.  This retrieves the *k* most relevant text chunks from your knowledge base (where *k* is a hyperparameter you can tune). Common similarity metrics include cosine similarity and dot product.\n",
       "*   **Augmentation (RAG):**\n",
       "    *   **Contextualization:** Combine the retrieved text chunks with the user's query.  This creates a context-rich input for the BERT model.  You can do this by:\n",
       "        *   **Concatenation:** Simply concatenate the query and the retrieved documents (with separators like `[SEP]` or `\\n\\n`).\n",
       "        *   **Prompt Engineering:**  Create a prompt that instructs BERT to answer the query based on the provided context. For example:  `\"Answer the following question based on the context provided. Question: {query} Context: {retrieved_text}\"`.  Good prompt engineering is *crucial* for RAG performance.\n",
       "*   **Generation:**\n",
       "    *   **Pass to BERT:** Feed the combined (query + context) input to a BERT model.  Crucially, you need to use a *generative* BERT model (or a model fine-tuned for text generation) to produce the chatbot's response. Options include:\n",
       "        *   **BERT2BERT:** An encoder-decoder model where both the encoder and decoder are based on BERT.\n",
       "        *   **BART (Bidirectional and Auto-Regressive Transformer):**  Good for sequence-to-sequence tasks like text generation.\n",
       "        *   **T5 (Text-to-Text Transfer Transformer):**  Trained on a massive dataset and can perform a wide range of text-based tasks.\n",
       "        *   **GPT-2/GPT-3/GPT-4 (via API):** While not strictly BERT, these models can be integrated into a RAG pipeline for powerful generation.  Consider the cost implications of using these APIs.\n",
       "    *   **Response Decoding:** Decode the output of the BERT model to generate the chatbot's text response.\n",
       "*   **Response Formatting:**\n",
       "    *   Format the response for readability.  You might want to add line breaks, bullet points, or other formatting elements.\n",
       "\n",
       "**3. Frameworks and Libraries**\n",
       "\n",
       "Several frameworks can simplify the development process:\n",
       "\n",
       "*   **LangChain:**  A powerful framework specifically designed for building applications with language models. It provides modules for data loading, text splitting, vector store integration, prompt engineering, and language model orchestration.  Highly recommended.\n",
       "*   **LlamaIndex:** Another framework like Langchain for integrating your data with LLMs.\n",
       "*   **Transformers (Hugging Face):** Essential for working with BERT and other transformer models. Provides pre-trained models, tokenizers, and utilities for fine-tuning.\n",
       "*   **Haystack:** An open-source framework for building search systems, including question answering systems.\n",
       "*   **Sentence Transformers:** A library specifically designed for generating high-quality sentence embeddings.\n",
       "\n",
       "**4. Code Example (Conceptual - using LangChain)**\n",
       "\n",
       "```python\n",
       "from langchain.embeddings import HuggingFaceEmbeddings\n",
       "from langchain.vectorstores import FAISS\n",
       "from langchain.llms import HuggingFaceHub  # Or OpenAI, etc.\n",
       "from langchain.chains import RetrievalQA\n",
       "from langchain.document_loaders import TextLoader\n",
       "\n",
       "# 1. Load and prepare your data\n",
       "loader = TextLoader(\"my_knowledge_base.txt\") # Replace with your data loading\n",
       "documents = loader.load()\n",
       "\n",
       "# 2. Create embeddings\n",
       "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\") # Choose a good sentence embedding model\n",
       "\n",
       "# 3. Create a vector store\n",
       "db = FAISS.from_documents(documents, embeddings)\n",
       "\n",
       "# 4. Initialize the language model (Choose a generative model)\n",
       "llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512}) # Example with FLAN-T5\n",
       "\n",
       "# 5. Create a RetrievalQA chain (RAG)\n",
       "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever(search_kwargs={\"k\": 3})) # 'stuff' is a simple chain type\n",
       "\n",
       "# 6. Chatbot loop\n",
       "while True:\n",
       "    query = input(\"Ask me anything: \")\n",
       "    if query.lower() == \"exit\":\n",
       "        break\n",
       "    result = qa({\"query\": query})\n",
       "    print(result[\"result\"])\n",
       "```\n",
       "\n",
       "**Explanation of the code:**\n",
       "\n",
       "*   **Data Loading:**  Loads your knowledge base (replace `\"my_knowledge_base.txt\"` with your actual data).\n",
       "*   **Embeddings:**  Creates sentence embeddings using `HuggingFaceEmbeddings`.\n",
       "*   **Vector Store:**  Indexes the embeddings in a FAISS vector store.\n",
       "*   **Language Model:**  Initializes a language model (in this case, FLAN-T5 hosted on Hugging Face Hub).  You'll need an API key for Hugging Face Hub.  You can also use OpenAI, but that requires a paid OpenAI API key.\n",
       "*   **RetrievalQA Chain:**  Creates a `RetrievalQA` chain that combines retrieval (from the vector store) and generation (with the language model). The `chain_type=\"stuff\"` argument specifies a simple way of combining the retrieved documents into the prompt.  `search_kwargs={\"k\": 3}` specifies the number of documents to retrieve.\n",
       "*   **Chatbot Loop:**  Gets user input, runs the query through the `RetrievalQA` chain, and prints the result.\n",
       "\n",
       "**5. Key Considerations and Challenges**\n",
       "\n",
       "*   **Computational Resources:** Training and running large BERT models can be computationally expensive. Consider using a GPU or cloud-based resources.\n",
       "*   **Fine-Tuning:** Fine-tuning the BERT model on your specific dataset can significantly improve performance, but requires a substantial amount of labeled data.\n",
       "*   **Context Length:**  BERT has a limited context length (e.g., 512 tokens).  Carefully consider how you split your documents and create your prompts to stay within this limit.  Longer context models exist, like Longformer or even newer models that extend beyond typical BERT models.\n",
       "*   **Hallucinations:**  Language models can sometimes generate incorrect or nonsensical answers (hallucinations). RAG can help reduce this by providing the model with grounded context.\n",
       "*   **Prompt Engineering:** Designing effective prompts is crucial for getting good results with RAG. Experiment with different prompts to find what works best for your task.\n",
       "*   **Scalability:**  For production deployments, you'll need to consider scalability.  Vector databases like Pinecone and Weaviate are designed to handle large datasets and high query volumes.\n",
       "*   **Evaluation:**  Thoroughly evaluate your chatbot's performance to identify areas for improvement.  Metrics like accuracy, relevance, and fluency are important.\n",
       "*   **Cost:**  Consider the cost of using cloud-based services like OpenAI, Pinecone, or Hugging Face Hub.\n",
       "\n",
       "**In summary, building a BERT-powered chatbot with RAG involves data preparation, embedding generation, vector database indexing, chatbot implementation, and careful consideration of performance and scalability. Using frameworks like LangChain and Hugging Face Transformers can greatly simplify the development process.** Good luck!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('How i can integrate a chatbot consist a bert model and Rag technique')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03356be5",
   "metadata": {
    "id": "2KAjpr1200sW",
    "papermill": {
     "duration": 0.009097,
     "end_time": "2025-04-07T21:05:35.314713",
     "exception": false,
     "start_time": "2025-04-07T21:05:35.305616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Choose a model\n",
    "\n",
    "The Gemini API provides access to a number of models from the Gemini model family. Read about the available models and their capabilities on the [model overview page](https://ai.google.dev/gemini-api/docs/models/gemini).\n",
    "\n",
    "In this step you'll use the API to list all of the available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef012881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:35.334177Z",
     "iopub.status.busy": "2025-04-07T21:05:35.333914Z",
     "iopub.status.idle": "2025-04-07T21:05:35.357792Z",
     "shell.execute_reply": "2025-04-07T21:05:35.356871Z"
    },
    "id": "uUUZa2uq2jDm",
    "papermill": {
     "duration": 0.035832,
     "end_time": "2025-04-07T21:05:35.359733",
     "exception": false,
     "start_time": "2025-04-07T21:05:35.323901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e1fb9",
   "metadata": {
    "id": "rN49kSI54R1v",
    "papermill": {
     "duration": 0.00903,
     "end_time": "2025-04-07T21:05:35.379079",
     "exception": false,
     "start_time": "2025-04-07T21:05:35.370049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The [`models.list`](https://ai.google.dev/api/models#method:-models.list) response also returns additional information about the model's capabilities, like the token limits and supported parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268eac7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:35.398904Z",
     "iopub.status.busy": "2025-04-07T21:05:35.398581Z",
     "iopub.status.idle": "2025-04-07T21:05:35.422590Z",
     "shell.execute_reply": "2025-04-07T21:05:35.421668Z"
    },
    "id": "k7JJ1K6j4Rl8",
    "papermill": {
     "duration": 0.035748,
     "end_time": "2025-04-07T21:05:35.424056",
     "exception": false,
     "start_time": "2025-04-07T21:05:35.388308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Gemini 2.0 Flash',\n",
      " 'display_name': 'Gemini 2.0 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_actions': ['generateContent', 'countTokens'],\n",
      " 'tuned_model_info': {},\n",
      " 'version': '2.0'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for model in client.models.list():\n",
    "  if model.name == 'models/gemini-2.0-flash':\n",
    "    pprint(model.to_json_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f923f2",
   "metadata": {
    "id": "5rU_UBlZdooM",
    "papermill": {
     "duration": 0.009059,
     "end_time": "2025-04-07T21:05:35.442582",
     "exception": false,
     "start_time": "2025-04-07T21:05:35.433523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Explore generation parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9731d6",
   "metadata": {
    "id": "G7NfEizeipbW",
    "papermill": {
     "duration": 0.008976,
     "end_time": "2025-04-07T21:05:35.460784",
     "exception": false,
     "start_time": "2025-04-07T21:05:35.451808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Output length\n",
    "\n",
    "When generating text with an LLM, the output length affects cost and performance. Generating more tokens increases computation, leading to higher energy consumption, latency, and cost.\n",
    "\n",
    "To stop the model from generating tokens past a limit, you can specify the `max_output_tokens` parameter when using the Gemini API. Specifying this parameter does not influence the generation of the output tokens, so the output will not become more stylistically or textually succinct, but it will stop generating tokens once the specified length is reached. Prompt engineering may be required to generate a more complete output for your given limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6f08ed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:35.480711Z",
     "iopub.status.busy": "2025-04-07T21:05:35.480388Z",
     "iopub.status.idle": "2025-04-07T21:05:36.920455Z",
     "shell.execute_reply": "2025-04-07T21:05:36.919445Z"
    },
    "id": "qVf23JsIi9ma",
    "papermill": {
     "duration": 1.451618,
     "end_time": "2025-04-07T21:05:36.921915",
     "exception": false,
     "start_time": "2025-04-07T21:05:35.470297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Olive's Enduring Embrace: Importance in Modern Society\n",
      "\n",
      "The olive, a humble fruit borne from the ancient Olea europaea, transcends its status as a simple ingredient. Its influence ripples through history, culture, and the modern world, impacting culinary landscapes, health paradigms, economic sectors, and even environmental consciousness. From the verdant groves of the Mediterranean to the sophisticated palates of global consumers, the olive maintains an enduring embrace on society, underscoring its vital importance in a multitude of ways.\n",
      "\n",
      "Firstly, the olive's impact on culinary traditions and contemporary cuisine is undeniable. For millennia, olives have been a cornerstone of Mediterranean gastronomy, a region recognized for its healthy and delicious diet. Eaten whole, brined, or pressed into oil, olives contribute unique flavors and textures to a vast array of dishes. Their inherent bitterness, tempered by curing processes, adds a depth and complexity that elevates simple fare. From the salty tang of Kalamata olives in a Greek salad to the\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "short_config = types.GenerateContentConfig(max_output_tokens=200)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='Write a 1000 word essay on the importance of olives in modern society.')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba55f454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:36.942444Z",
     "iopub.status.busy": "2025-04-07T21:05:36.942107Z",
     "iopub.status.idle": "2025-04-07T21:05:37.630079Z",
     "shell.execute_reply": "2025-04-07T21:05:37.628870Z"
    },
    "id": "W-3kR2F5kdMR",
    "papermill": {
     "duration": 0.70014,
     "end_time": "2025-04-07T21:05:37.631839",
     "exception": false,
     "start_time": "2025-04-07T21:05:36.931699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From grove to table, small and green,\n",
      "An olive's tale, a modern scene.\n",
      "In salads bright, a savory bite,\n",
      "Pressed into oil, a golden light.\n",
      "\n",
      "On pizza tossed, or tapenade,\n",
      "A taste of sun, a life well-made.\n",
      "From simple snack to gourmet spread,\n",
      "The olive reigns, a blessing spread.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='Write a short poem on the importance of olives in modern society.')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43278c6",
   "metadata": {
    "id": "3ZhDSLB6lqqB",
    "papermill": {
     "duration": 0.009382,
     "end_time": "2025-04-07T21:05:37.652910",
     "exception": false,
     "start_time": "2025-04-07T21:05:37.643528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Explore with your own prompts. Try a prompt with a restrictive output limit and then adjust the prompt to work within that limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2640f64e",
   "metadata": {
    "id": "alx-WaAvir_9",
    "papermill": {
     "duration": 0.009569,
     "end_time": "2025-04-07T21:05:37.671939",
     "exception": false,
     "start_time": "2025-04-07T21:05:37.662370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
    "\n",
    "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a85f61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:37.692013Z",
     "iopub.status.busy": "2025-04-07T21:05:37.691739Z",
     "iopub.status.idle": "2025-04-07T21:05:38.992091Z",
     "shell.execute_reply": "2025-04-07T21:05:38.991383Z"
    },
    "id": "SHraGMzqnZqt",
    "papermill": {
     "duration": 1.312069,
     "end_time": "2025-04-07T21:05:38.993430",
     "exception": false,
     "start_time": "2025-04-07T21:05:37.681361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "high_temp_config = types.GenerateContentConfig(temperature=0.2)\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=high_temp_config,\n",
    "      contents='Pick a random colour... (respond in a single word)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ce964",
   "metadata": {
    "id": "z3J4pCTuof7e",
    "papermill": {
     "duration": 0.009411,
     "end_time": "2025-04-07T21:05:39.013005",
     "exception": false,
     "start_time": "2025-04-07T21:05:39.003594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now try the same prompt with temperature set to zero. Note that the output is not completely deterministic, as other parameters affect token selection, but the results will tend to be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8548386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:39.033529Z",
     "iopub.status.busy": "2025-04-07T21:05:39.033223Z",
     "iopub.status.idle": "2025-04-07T21:05:40.120273Z",
     "shell.execute_reply": "2025-04-07T21:05:40.119406Z"
    },
    "id": "clymkWv-PfUZ",
    "papermill": {
     "duration": 1.099768,
     "end_time": "2025-04-07T21:05:40.122385",
     "exception": false,
     "start_time": "2025-04-07T21:05:39.022617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_config = types.GenerateContentConfig(temperature=0.0)\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=low_temp_config,\n",
    "      contents='Pick a random colour... (respond in a single word)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642088e1",
   "metadata": {
    "id": "St5nt3vzitsZ",
    "papermill": {
     "duration": 0.009392,
     "end_time": "2025-04-07T21:05:40.142083",
     "exception": false,
     "start_time": "2025-04-07T21:05:40.132691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Top-P\n",
    "\n",
    "Like temperature, the top-P parameter is also used to control the diversity of the model's output.\n",
    "\n",
    "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
    "\n",
    "You may also see top-K referenced in LLM literature. Top-K is not configurable in the Gemini 2.0 series of models, but can be changed in older models. Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
    "\n",
    "\n",
    "Run this example a number of times, change the settings and observe the change in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20de3d0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:40.162793Z",
     "iopub.status.busy": "2025-04-07T21:05:40.162410Z",
     "iopub.status.idle": "2025-04-07T21:05:45.624273Z",
     "shell.execute_reply": "2025-04-07T21:05:45.623345Z"
    },
    "id": "lPlzpEavUV8F",
    "papermill": {
     "duration": 5.474248,
     "end_time": "2025-04-07T21:05:45.626098",
     "exception": false,
     "start_time": "2025-04-07T21:05:40.151850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clementine, a calico with a perpetually surprised expression, was not built for adventure. She preferred sunbeams, soft cushions, and the rhythmic scratch of her human, Eleanor, behind her ears. But adventure, it seemed, had other plans for Clementine.\n",
      "\n",
      "It started with a robin. Not just any robin, mind you, but a particularly audacious one, perched just outside Eleanor's open window, flaunting a glistening earthworm. Clementine, roused from her nap by the worm's wriggling, felt a primal urge ignite within her. She had to have it.\n",
      "\n",
      "With a leap and a scramble, she was out the window, landing with a less-than-graceful thump on the flower-laden balcony. The robin, startled, fluttered to a nearby oak tree. Clementine, undeterred, followed.\n",
      "\n",
      "This was new territory. The familiar scent of Eleanor's garden faded as she cautiously navigated the rough bark of the oak. The robin, leading her on, hopped from branch to branch, deeper and deeper into the unknown wilderness of Eleanor's sprawling backyard.\n",
      "\n",
      "The backyard was a jungle to a pampered housecat. Towering sunflowers became looming giants. The babbling brook, usually a distant murmur, was now a rushing river she had to carefully leap across. She dodged buzzing bees the size of her head (or so they seemed), and navigated through a maze of fragrant lavender bushes.\n",
      "\n",
      "Suddenly, the robin took flight, disappearing into a thicket of tangled blackberry bushes. Clementine, frustrated, peered into the thorny depths. This was a challenge. The worm, however, remained a compelling prize.\n",
      "\n",
      "Ignoring the sharp pricks against her fur, she bravely plunged in. The blackberries scratched and pulled, the air thick with the sweet, tart scent of ripened fruit. She squeezed and wriggled, determined to reach the other side.\n",
      "\n",
      "And then, she saw it. Not the robin, but something far more intriguing. A tiny, hidden glade, bathed in dappled sunlight. In the center stood a miniature toadstool circle, shimmering with an ethereal glow.\n",
      "\n",
      "Clementine, forgetting all about the robin and its worm, approached the circle cautiously. As she stepped inside, a strange tingling sensation washed over her. The air hummed with an invisible energy.\n",
      "\n",
      "She didn't understand what it was, but she knew it was magical. It felt like sunshine on her fur, like Eleanor's gentle touch, but amplified, imbued with a sense of wonder. She sat in the center of the circle, soaking it in.\n",
      "\n",
      "Finally, the tingling subsided. Clementine stretched, feeling strangely refreshed and invigorated. She had no worm, but she had found something far more valuable: a secret, a magical place hidden in her own backyard.\n",
      "\n",
      "Making her way back through the blackberry bushes, she realized she wasn't afraid anymore. The buzzing bees, the rushing brook, the towering sunflowers ‚Äì they no longer seemed so daunting. She had faced them, conquered them, and discovered a hidden magic within herself.\n",
      "\n",
      "When she finally returned to Eleanor, covered in scratches and smelling of blackberries, Eleanor gasped. \"Clementine! Where have you been?\"\n",
      "\n",
      "Clementine simply purred, rubbing against Eleanor's legs. She wouldn't tell her about the robin, or the worm, or the magical toadstool circle. Some adventures were best kept secret, tucked away in the heart, waiting to be relived in dreams.\n",
      "\n",
      "And from then on, Clementine, the calico who wasn't built for adventure, found herself looking at the world with a new appreciation, a spark of curiosity twinkling in her emerald eyes. For she knew, now, that adventure could be found in the most unexpected of places, even in her own backyard. And all it took was a little bit of courage, a little bit of curiosity, and a very tempting earthworm.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    # These are the default values for gemini-2.0-flash.\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=story_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6e60a",
   "metadata": {
    "id": "rMrYs1koY6DX",
    "papermill": {
     "duration": 0.009709,
     "end_time": "2025-04-07T21:05:45.646322",
     "exception": false,
     "start_time": "2025-04-07T21:05:45.636613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prompting\n",
    "\n",
    "This section contains some prompts from the chapter for you to try out directly in the API. Try changing the text here to see how each prompt performs with different instructions, more examples, or any other changes you can think of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79e051",
   "metadata": {
    "id": "hhj_tQidZJP7",
    "papermill": {
     "duration": 0.009635,
     "end_time": "2025-04-07T21:05:45.665912",
     "exception": false,
     "start_time": "2025-04-07T21:05:45.656277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Zero-shot\n",
    "\n",
    "Zero-shot prompts are prompts that describe the request for the model directly.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcbf651a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:45.687031Z",
     "iopub.status.busy": "2025-04-07T21:05:45.686751Z",
     "iopub.status.idle": "2025-04-07T21:05:46.092596Z",
     "shell.execute_reply": "2025-04-07T21:05:46.091712Z"
    },
    "id": "1_t-cwnDZzbH",
    "papermill": {
     "duration": 0.418381,
     "end_time": "2025-04-07T21:05:46.094282",
     "exception": false,
     "start_time": "2025-04-07T21:05:45.675901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=5,\n",
    ")\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3637e2",
   "metadata": {
    "id": "3b5568bdeb11",
    "papermill": {
     "duration": 0.009646,
     "end_time": "2025-04-07T21:05:46.114573",
     "exception": false,
     "start_time": "2025-04-07T21:05:46.104927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Enum mode\n",
    "\n",
    "The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards. See [this prompt in AI Studio](https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g) for an example.\n",
    "\n",
    "The Gemini API has an [Enum mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Enum.ipynb) feature that allows you to constrain the output to a fixed set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f612259e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:46.135406Z",
     "iopub.status.busy": "2025-04-07T21:05:46.135159Z",
     "iopub.status.idle": "2025-04-07T21:05:46.720587Z",
     "shell.execute_reply": "2025-04-07T21:05:46.719748Z"
    },
    "id": "ad118a56c598",
    "papermill": {
     "duration": 0.597565,
     "end_time": "2025-04-07T21:05:46.721970",
     "exception": false,
     "start_time": "2025-04-07T21:05:46.124405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ),\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d67a1",
   "metadata": {
    "papermill": {
     "duration": 0.009991,
     "end_time": "2025-04-07T21:05:46.742637",
     "exception": false,
     "start_time": "2025-04-07T21:05:46.732646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When using constrained output like an enum, the Python SDK will attempt to convert the model's text response into a Python object automatically. It's stored in the `response.parsed` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "573e579a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:46.763932Z",
     "iopub.status.busy": "2025-04-07T21:05:46.763646Z",
     "iopub.status.idle": "2025-04-07T21:05:46.767931Z",
     "shell.execute_reply": "2025-04-07T21:05:46.767216Z"
    },
    "papermill": {
     "duration": 0.016545,
     "end_time": "2025-04-07T21:05:46.769230",
     "exception": false,
     "start_time": "2025-04-07T21:05:46.752685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.POSITIVE\n",
      "<enum 'Sentiment'>\n"
     ]
    }
   ],
   "source": [
    "enum_response = response.parsed\n",
    "print(enum_response)\n",
    "print(type(enum_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e61562",
   "metadata": {
    "id": "0udiSwNbv45W",
    "papermill": {
     "duration": 0.009836,
     "end_time": "2025-04-07T21:05:46.789534",
     "exception": false,
     "start_time": "2025-04-07T21:05:46.779698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### One-shot and few-shot\n",
    "\n",
    "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1jjWkjUSoMXmLvMJ7IzADr_GxHPJVV2bg\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "438ca004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:46.811090Z",
     "iopub.status.busy": "2025-04-07T21:05:46.810782Z",
     "iopub.status.idle": "2025-04-07T21:05:47.368271Z",
     "shell.execute_reply": "2025-04-07T21:05:47.367512Z"
    },
    "id": "hd4mVUukwOKZ",
    "papermill": {
     "duration": 0.570256,
     "end_time": "2025-04-07T21:05:47.369878",
     "exception": false,
     "start_time": "2025-04-07T21:05:46.799622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ),\n",
    "    contents=[few_shot_prompt, customer_order])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123073d",
   "metadata": {
    "id": "021293096f08",
    "papermill": {
     "duration": 0.009791,
     "end_time": "2025-04-07T21:05:47.390139",
     "exception": false,
     "start_time": "2025-04-07T21:05:47.380348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### JSON mode\n",
    "\n",
    "To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's [JSON mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb). This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84462d09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:05:47.411883Z",
     "iopub.status.busy": "2025-04-07T21:05:47.411577Z",
     "iopub.status.idle": "2025-04-07T21:06:58.351867Z",
     "shell.execute_reply": "2025-04-07T21:06:58.351021Z"
    },
    "id": "50fbf0260912",
    "papermill": {
     "duration": 70.963345,
     "end_time": "2025-04-07T21:06:58.363802",
     "exception": false,
     "start_time": "2025-04-07T21:05:47.400457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
      "  \"type\": \"dessert\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ),\n",
    "    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a483eb",
   "metadata": {
    "id": "4a93e338e57c",
    "papermill": {
     "duration": 0.009861,
     "end_time": "2025-04-07T21:06:58.383756",
     "exception": false,
     "start_time": "2025-04-07T21:06:58.373895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Chain of Thought (CoT)\n",
    "\n",
    "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
    "\n",
    "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
    "\n",
    "Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad85f58a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:06:58.405304Z",
     "iopub.status.busy": "2025-04-07T21:06:58.405014Z",
     "iopub.status.idle": "2025-04-07T21:06:58.852066Z",
     "shell.execute_reply": "2025-04-07T21:06:58.851156Z"
    },
    "id": "5715555db1c1",
    "papermill": {
     "duration": 0.459554,
     "end_time": "2025-04-07T21:06:58.853540",
     "exception": false,
     "start_time": "2025-04-07T21:06:58.393986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0711ba5",
   "metadata": {
    "id": "e12b19677bfd",
    "papermill": {
     "duration": 0.00987,
     "end_time": "2025-04-07T21:06:58.874028",
     "exception": false,
     "start_time": "2025-04-07T21:06:58.864158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now try the same approach, but indicate to the model that it should \"think step by step\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e29aaba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:06:58.895663Z",
     "iopub.status.busy": "2025-04-07T21:06:58.895322Z",
     "iopub.status.idle": "2025-04-07T21:06:59.671550Z",
     "shell.execute_reply": "2025-04-07T21:06:59.670690Z"
    },
    "id": "ffd7536a481f",
    "papermill": {
     "duration": 0.788946,
     "end_time": "2025-04-07T21:06:59.673050",
     "exception": false,
     "start_time": "2025-04-07T21:06:58.884104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem step-by-step:\n",
       "\n",
       "1.  **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
       "\n",
       "2.  **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "\n",
       "3.  **Determine your partner's current age:** Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
       "\n",
       "**Answer:** Your partner is currently 28 years old."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab478e61",
   "metadata": {
    "id": "oiLgBQJj0V53",
    "papermill": {
     "duration": 0.010105,
     "end_time": "2025-04-07T21:06:59.693752",
     "exception": false,
     "start_time": "2025-04-07T21:06:59.683647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ReAct: Reason and act\n",
    "\n",
    "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper.\n",
    "\n",
    "To try this out with the Wikipedia search engine, check out the [Searching Wikipedia with ReAct](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) cookbook example.\n",
    "\n",
    "\n",
    "> Note: The prompt and in-context examples used here are from [https://github.com/ysymyth/ReAct](https://github.com/ysymyth/ReAct) which is published under an [MIT license](https://opensource.org/licenses/MIT), Copyright (c) 2023 Shunyu Yao.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/18oo63Lwosd-bQ6Ay51uGogB3Wk3H8XMO\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e2e9b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:06:59.715714Z",
     "iopub.status.busy": "2025-04-07T21:06:59.715434Z",
     "iopub.status.idle": "2025-04-07T21:06:59.720183Z",
     "shell.execute_reply": "2025-04-07T21:06:59.719559Z"
    },
    "id": "cBgyNJ5z0VSs",
    "papermill": {
     "duration": 0.017505,
     "end_time": "2025-04-07T21:06:59.721603",
     "exception": false,
     "start_time": "2025-04-07T21:06:59.704098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\"\n",
    "\n",
    "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed778e",
   "metadata": {
    "id": "C3wbfstjTgey",
    "papermill": {
     "duration": 0.009909,
     "end_time": "2025-04-07T21:06:59.742125",
     "exception": false,
     "start_time": "2025-04-07T21:06:59.732216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To capture a single step at a time, while ignoring any hallucinated Observation steps, you will use `stop_sequences` to end the generation process. The steps are `Thought`, `Action`, `Observation`, in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4030223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:06:59.763951Z",
     "iopub.status.busy": "2025-04-07T21:06:59.763655Z",
     "iopub.status.idle": "2025-04-07T21:07:00.361611Z",
     "shell.execute_reply": "2025-04-07T21:07:00.360538Z"
    },
    "id": "8mxrXRkRTdXm",
    "papermill": {
     "duration": 0.610957,
     "end_time": "2025-04-07T21:07:00.363277",
     "exception": false,
     "start_time": "2025-04-07T21:06:59.752320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the transformers NLP paper, identify the authors, and determine who the youngest author is.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "# You will perform the Action; so generate up to, but not including, the Observation.\n",
    "react_config = types.GenerateContentConfig(\n",
    "    stop_sequences=[\"\\nObservation\"],\n",
    "    system_instruction=model_instructions + example1 + example2,\n",
    ")\n",
    "\n",
    "# Create a chat that has the model instructions and examples pre-seeded.\n",
    "react_chat = client.chats.create(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=react_config,\n",
    ")\n",
    "\n",
    "resp = react_chat.send_message(question)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d76c7",
   "metadata": {
    "id": "aW2PIdLbVv3l",
    "papermill": {
     "duration": 0.010048,
     "end_time": "2025-04-07T21:07:00.384098",
     "exception": false,
     "start_time": "2025-04-07T21:07:00.374050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now you can perform this research yourself and supply it back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbec63f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:07:00.405995Z",
     "iopub.status.busy": "2025-04-07T21:07:00.405714Z",
     "iopub.status.idle": "2025-04-07T21:07:01.176819Z",
     "shell.execute_reply": "2025-04-07T21:07:01.175948Z"
    },
    "id": "mLMc0DZaV9g2",
    "papermill": {
     "duration": 0.783988,
     "end_time": "2025-04-07T21:07:01.178324",
     "exception": false,
     "start_time": "2025-04-07T21:07:00.394336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "Now I have the list of authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to find the birthdates of each author. It would be difficult to find and verify all their ages. I will start with the first author on the list.\n",
      "\n",
      "Action 2\n",
      "<search>Ashish Vaswani</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae902912",
   "metadata": {
    "id": "bo0tzf4nX6dA",
    "papermill": {
     "duration": 0.010238,
     "end_time": "2025-04-07T21:07:01.199332",
     "exception": false,
     "start_time": "2025-04-07T21:07:01.189094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This process repeats until the `<finish>` action is reached. You can continue running this yourself if you like, or try the [Wikipedia example](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) to see a fully automated ReAct system at work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5244e",
   "metadata": {
    "papermill": {
     "duration": 0.01021,
     "end_time": "2025-04-07T21:07:01.219852",
     "exception": false,
     "start_time": "2025-04-07T21:07:01.209642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Thinking mode\n",
    "\n",
    "The experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n",
    "\n",
    "Using a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated.\n",
    "\n",
    "Note that when you use the API, you get the final response from the model, but the thoughts are not captured. To see the intermediate thoughts, try out [the thinking mode model in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-thinking-exp-01-21).\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1Z991SV7lZZZqioOiqIUPv9a9ix-ws4zk\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58f34157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:07:01.242260Z",
     "iopub.status.busy": "2025-04-07T21:07:01.241903Z",
     "iopub.status.idle": "2025-04-07T21:07:20.992917Z",
     "shell.execute_reply": "2025-04-07T21:07:20.992051Z"
    },
    "papermill": {
     "duration": 19.764279,
     "end_time": "2025-04-07T21:07:20.994323",
     "exception": false,
     "start_time": "2025-04-07T21:07:01.230044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the \"Attention is All You Need\" paper (which is widely considered *the* foundational Transformer paper), the youngest author is likely **Aidan N. Gomez**.\n",
       "\n",
       "Here's why:\n",
       "\n",
       "* **Affiliation at the time of publication:**  At the time the paper was published (2017), Aidan N. Gomez was affiliated with the University of Toronto and was a PhD student.  PhD students are typically younger than researchers in established positions at companies like Google (where many of the other authors were affiliated at the time).\n",
       "\n",
       "* **Career Stage:**  Looking at author profiles and publication history, Aidan N. Gomez appears to be earlier in his career compared to the other authors who have longer publication records and more senior roles at Google Brain or other research institutions.\n",
       "\n",
       "While we don't have precise birthdates for all authors to definitively say who is *the absolute* youngest, based on typical career trajectories and affiliations at the time of publication, **Aidan N. Gomez is the most likely to be the youngest author on the \"Attention is All You Need\" paper.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model='gemini-2.0-flash-thinking-exp',\n",
    "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
    ")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    # Display the response as it is streamed\n",
    "    print(chunk.text, end='')\n",
    "\n",
    "# And then render the finished response as formatted markdown.\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513afd18",
   "metadata": {
    "id": "jPiZ_eIIaVPt",
    "papermill": {
     "duration": 0.010034,
     "end_time": "2025-04-07T21:07:21.015113",
     "exception": false,
     "start_time": "2025-04-07T21:07:21.005079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Code prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905590c7",
   "metadata": {
    "id": "ZinKamwXeR6C",
    "papermill": {
     "duration": 0.010017,
     "end_time": "2025-04-07T21:07:21.035337",
     "exception": false,
     "start_time": "2025-04-07T21:07:21.025320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generating code\n",
    "\n",
    "The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.\n",
    "\n",
    "It's important to be aware that since LLMs can make mistakes, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1YX71JGtzDjXQkgdes8bP6i3oH5lCRKxv\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88e3272e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:07:21.057031Z",
     "iopub.status.busy": "2025-04-07T21:07:21.056738Z",
     "iopub.status.idle": "2025-04-07T21:07:21.671279Z",
     "shell.execute_reply": "2025-04-07T21:07:21.670365Z"
    },
    "id": "fOQP9pqmeUO1",
    "papermill": {
     "duration": 0.627196,
     "end_time": "2025-04-07T21:07:21.672759",
     "exception": false,
     "start_time": "2025-04-07T21:07:21.045563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "    if n == 0:\n",
       "        return 1\n",
       "    else:\n",
       "        return n * factorial(n-1)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Gemini models love to talk, so it helps to specify they stick to the code if that\n",
    "# is all that you want.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ),\n",
    "    contents=code_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8834f77",
   "metadata": {
    "id": "wlBMWSFhgVRQ",
    "papermill": {
     "duration": 0.010249,
     "end_time": "2025-04-07T21:07:21.694235",
     "exception": false,
     "start_time": "2025-04-07T21:07:21.683986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Code execution\n",
    "\n",
    "The Gemini API can automatically run generated code too, and will return the output.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/11veFr_VYEwBWcLkhNLr-maCG0G8sS_7Z\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96507dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:07:21.717035Z",
     "iopub.status.busy": "2025-04-07T21:07:21.716740Z",
     "iopub.status.idle": "2025-04-07T21:07:23.268433Z",
     "shell.execute_reply": "2025-04-07T21:07:23.267461Z"
    },
    "id": "jT3OfWYfhjRL",
    "papermill": {
     "duration": 1.564841,
     "end_time": "2025-04-07T21:07:23.269887",
     "exception": false,
     "start_time": "2025-04-07T21:07:21.705046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Okay, I understand. First, I'll generate the first 14 odd prime \"\n",
      "         \"numbers. Then, I'll calculate their sum.\\n\"\n",
      "         '\\n'}\n",
      "-----\n",
      "{'executable_code': {'code': 'primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, '\n",
      "                             '37, 41, 43, 47]\\n'\n",
      "                             'sum_of_primes = sum(primes)\\n'\n",
      "                             \"print(f'{primes=}')\\n\"\n",
      "                             \"print(f'{sum_of_primes=}')\\n\",\n",
      "                     'language': 'PYTHON'}}\n",
      "-----\n",
      "{'code_execution_result': {'outcome': 'OUTCOME_OK',\n",
      "                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "                                     '31, 37, 41, 43, 47]\\n'\n",
      "                                     'sum_of_primes=326\\n'}}\n",
      "-----\n",
      "{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "         '31, 37, 41, 43, and 47. Their sum is 326.\\n'}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n",
    ")\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Generate the first 14 odd prime numbers, then calculate their sum.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=config,\n",
    "    contents=code_exec_prompt)\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "  pprint(part.to_json_dict())\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e8fd44",
   "metadata": {
    "id": "ZspT1GSkjG6d",
    "papermill": {
     "duration": 0.010458,
     "end_time": "2025-04-07T21:07:23.291366",
     "exception": false,
     "start_time": "2025-04-07T21:07:23.280908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This response contains multiple parts, including an opening and closing text part that represent regular responses, an `executable_code` part that represents generated code and a `code_execution_result` part that represents the results from running the generated code.\n",
    "\n",
    "You can explore them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c90ce3c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:07:23.314365Z",
     "iopub.status.busy": "2025-04-07T21:07:23.313724Z",
     "iopub.status.idle": "2025-04-07T21:07:23.323653Z",
     "shell.execute_reply": "2025-04-07T21:07:23.322956Z"
    },
    "papermill": {
     "duration": 0.022913,
     "end_time": "2025-04-07T21:07:23.325224",
     "exception": false,
     "start_time": "2025-04-07T21:07:23.302311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I understand. First, I'll generate the first 14 odd prime numbers. Then, I'll calculate their sum.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum_of_primes = sum(primes)\n",
       "print(f'{primes=}')\n",
       "print(f'{sum_of_primes=}')\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum_of_primes=326\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47. Their sum is 326.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text:\n",
    "        display(Markdown(part.text))\n",
    "    elif part.executable_code:\n",
    "        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
    "    elif part.code_execution_result:\n",
    "        if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
    "            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n",
    "\n",
    "        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fe29a",
   "metadata": {
    "id": "1gUX8QzCj4d5",
    "papermill": {
     "duration": 0.010529,
     "end_time": "2025-04-07T21:07:23.347616",
     "exception": false,
     "start_time": "2025-04-07T21:07:23.337087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Explaining code\n",
    "\n",
    "The Gemini family of models can explain code to you too. In this example, you pass a [bash script](https://github.com/magicmonty/bash-git-prompt) and ask some questions.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1N7LGzWzCYieyOf_7bAG4plrmkpDNmUyb\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "880a8cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:07:23.370271Z",
     "iopub.status.busy": "2025-04-07T21:07:23.369971Z",
     "iopub.status.idle": "2025-04-07T21:07:26.871376Z",
     "shell.execute_reply": "2025-04-07T21:07:26.870707Z"
    },
    "id": "7_jPMMoxkIEb",
    "papermill": {
     "duration": 3.515047,
     "end_time": "2025-04-07T21:07:26.873268",
     "exception": false,
     "start_time": "2025-04-07T21:07:23.358221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file is a Bash script designed to enhance your command-line prompt with Git repository status information. In a nutshell, it modifies your prompt to display details about the Git repository you're currently working in, such as the branch name, whether there are uncommitted changes, and the status of your remote connection.\n",
       "\n",
       "Here's a breakdown of what it does and why you'd use it:\n",
       "\n",
       "**What it does:**\n",
       "\n",
       "*   **Displays Git Status:** The primary function is to show information about your Git repository directly in your command prompt. This includes things like:\n",
       "    *   The current branch name\n",
       "    *   Whether there are staged or unstaged changes\n",
       "    *   Whether your local branch is ahead or behind the remote branch\n",
       "    *   The number of commits ahead/behind\n",
       "    *   The presence of untracked files\n",
       "    *   The status of submodules\n",
       "\n",
       "*   **Customizable:** It's designed to be configurable through environment variables and theme files. You can customize the colors, symbols, and the specific information displayed in the prompt. It allows creation of custom themes, that use colors.\n",
       "\n",
       "*   **Asynchronous Operations:** It attempts to fetch remote Git status asynchronously to avoid blocking your prompt while waiting for network operations. This way, prompt loading won't be blocked by network latency.\n",
       "\n",
       "*   **Virtual Environment Support:** Includes integration for displaying virtual environment information (Python virtualenv, Node.js, conda) in the prompt.\n",
       "\n",
       "**Why you'd use it:**\n",
       "\n",
       "*   **Increased Awareness:** It gives you immediate, visual feedback about the state of your Git repository, making it easier to track changes and manage your workflow. You don't have to type `git status` repeatedly.\n",
       "\n",
       "*   **Improved Productivity:** By having Git information readily available in the prompt, you can make quicker decisions and avoid common mistakes, such as committing to the wrong branch or forgetting to stage changes.\n",
       "\n",
       "*   **Customization:** You can tailor the prompt to show exactly the information you care about, and in a way that fits your personal preferences.\n",
       "\n",
       "*   **Efficiency:** It reduces the need to constantly run `git status` commands, saving you time and keystrokes.\n",
       "\n",
       "In summary, this script is a tool to make working with Git from the command line more efficient and informative by integrating Git status directly into your shell prompt. If you work with Git a lot in the terminal, it can be a valuable addition to your setup.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=explain_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a0dc6",
   "metadata": {
    "id": "2a8266d97ce5",
    "papermill": {
     "duration": 0.011191,
     "end_time": "2025-04-07T21:07:26.902516",
     "exception": false,
     "start_time": "2025-04-07T21:07:26.891325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Learn more\n",
    "\n",
    "To learn more about prompting in depth:\n",
    "\n",
    "* Check out the whitepaper issued with today's content,\n",
    "* Try out the apps listed at the top of this notebook ([TextFX](https://textfx.withgoogle.com/), [SQL Talk](https://sql-talk-r5gdynozbq-uc.a.run.app/) and [NotebookLM](https://notebooklm.google/)),\n",
    "* Read the [Introduction to Prompting](https://ai.google.dev/gemini-api/docs/prompting-intro) from the Gemini API docs,\n",
    "* Explore the Gemini API's [prompt gallery](https://ai.google.dev/gemini-api/prompts) and try them out in AI Studio,\n",
    "* Check out the Gemini API cookbook for [inspirational examples](https://github.com/google-gemini/cookbook/blob/main/examples/) and [educational quickstarts](https://github.com/google-gemini/cookbook/blob/main/quickstarts/).\n",
    "\n",
    "Be sure to check out the codelabs on day 3 too, where you will explore some more advanced prompting with code execution.\n",
    "\n",
    "And please share anything exciting you have tried in the Discord!\n",
    "\n",
    "*- [Mark McD](https://linktr.ee/markmcd)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-1-prompting.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 162.686351,
   "end_time": "2025-04-07T21:07:27.432585",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-07T21:04:44.746234",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
